{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autovit Data Scraper\n",
        "\n",
        "This notebook can scrape text data (such as the price of a car, or its model) from a fixed list of URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "from typing import Dict, List, Optional\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import pandas as pd\n",
        "import ratelimit\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The file which contains the URLs to scrape.\n",
        "LINKS_FILE = os.path.join(\"..\", \"..\", \"data\", \"all_cars.csv\")\n",
        "# The output filename (also used for checkpointing).\n",
        "DATAFRAME_FILE = os.path.join(\"..\", \"..\", \"data\", \"carsWithImages.csv\")\n",
        "\n",
        "# The first and last URLs to scrape (indices inside the URL list).\n",
        "LINKS_START = 16_000\n",
        "LINKS_STOP = 16_002\n",
        "\n",
        "# How often do we save the data to disk (once every `CHECKPOINT_RATE` entries).\n",
        "CHECKPOINT_RATE = 32\n",
        "\n",
        "# Maximum number of requests in a `LIMIT_PERIOD` period.\n",
        "LIMIT_CALLS: int = 1\n",
        "# Duration of a period, in seconds.\n",
        "LIMIT_PERIOD: int = 2\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for handling saving and reloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Save the scraped data to disk.\"\"\"\n",
        "    df.to_csv(DATAFRAME_FILE)\n",
        "\n",
        "\n",
        "def load_checkpoint() -> pd.DataFrame:\n",
        "    \"\"\"Load the latest version of the scraped data from disk.\"\"\"\n",
        "    return pd.read_csv(DATAFRAME_FILE, index_col=0)\n",
        "\n",
        "\n",
        "def checkpoint_exists() -> bool:\n",
        "    \"\"\"Check if previously scraped data exists on disk.\"\"\"\n",
        "    return os.path.isfile(DATAFRAME_FILE)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for downloading data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_webpage(url: str) -> str:\n",
        "    \"\"\"Download a text page from a given URL.\"\"\"\n",
        "    resp = urlopen(url).read().decode('utf-8')\n",
        "    return resp\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Requests are rate-limited to avoid overwhelming the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "@ratelimit.sleep_and_retry\n",
        "@ratelimit.limits(calls=LIMIT_CALLS, period=LIMIT_PERIOD)\n",
        "def check_autovit_limit() -> None:\n",
        "    \"\"\"\n",
        "    Utility function which forces requests to Autovit to respect rate limits.\n",
        "    This function should be used as a \"guard\", simply call it before you make a\n",
        "    request.\n",
        "    \"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for parsing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_webpage(html: str, url: str) -> Dict[str, str]:\n",
        "    \"\"\"Parse the useful information from an HTML page.\"\"\"\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    body = soup.body\n",
        "    siteWrap = body.find(\"div\", {\"id\":\"siteWrap\"})\n",
        "    flexContainerMain = siteWrap.find(\"div\", {\"class\":\"flex-container-main\"})\n",
        "    flexContainerMainLeft = flexContainerMain.find(\"div\", {\"class\":\"flex-container-main__left\"})\n",
        "    flexContainerMainRight = flexContainerMain.find(\"div\", {\"class\":\"flex-container-main__right\"})\n",
        "\n",
        "    offerContentAsside = flexContainerMainRight.find(\"div\", {\"class\":\"offer-content__aside\"})\n",
        "    offerSummary = offerContentAsside.find(\"div\",{\"class\":\"offer-summary\"})\n",
        "    priceWrapper = offerSummary.find(\"div\", {\"class\":\"price-wrapper\"})\n",
        "    offerPrice = priceWrapper.find(\"div\", {\"class\":\"offer-price\"})\n",
        "    offerContent = flexContainerMainLeft.find(\"div\", {\"class\":\"offer-content offer-content--secondary\"})\n",
        "    offerContentRaw = offerContent.find(\"div\", {\"class\":\"offer-content__row om-offer-main\"})\n",
        "    offerContentRawMain = offerContentRaw.find(\"div\", {\"class\":\"offer-content__main-column\"})\n",
        "    parametersArea = offerContentRawMain.find(\"div\", {\"class\":\"parametersArea\"})\n",
        "    parameters = parametersArea.find(\"div\", {\"id\": \"parameters\"})\n",
        "\n",
        "    price_sum = offerPrice[\"data-price\"]\n",
        "    price_currency = offerPrice.find(\"span\", {\"class\": \"offer-price__currency\"}).text\n",
        "    myDic = {\n",
        "        'Url': url,\n",
        "        'Autovit Id': soup.find(\"span\", {\"id\": \"ad_id\"}).text,\n",
        "        'Pret': f\"{price_sum} {price_currency}\",\n",
        "    }\n",
        "\n",
        "    for listCaract in parameters.find_all_next(\"ul\"):\n",
        "        for values in listCaract.find_all_next(\"li\"):\n",
        "            valuesParam =values.find(\"div\",{\"class\":\"offer-params__value\"})\n",
        "            valuesName = values.find(\"span\",{\"class\":\"offer-params__label\"})\n",
        "            try:\n",
        "                if valuesParam.a != None:\n",
        "                    myDic[valuesName.string] = valuesParam.a.string.strip()\n",
        "                else:\n",
        "                    myDic[valuesName.string] = valuesParam.string.strip()\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return myDic\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The actual scraping process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_url(url: str) -> Optional[Dict[str, str]]:\n",
        "    \"\"\"Download and parse a given web page.\"\"\"\n",
        "    try:\n",
        "        check_autovit_limit()\n",
        "        html = download_webpage(url)\n",
        "        return parse_webpage(html, url)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "def append_to_dataset(dictionary: Dict[str, str], df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add a new entry to the dataset.\"\"\"\n",
        "    return pd.concat([df, pd.DataFrame([dictionary])], ignore_index=True)\n",
        "\n",
        "\n",
        "def scrape_all(df: pd.DataFrame, urls: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"Scrape all the given URLs, printing debug information along the way.\"\"\"\n",
        "    for i, url in enumerate(urls):\n",
        "        display_name = f\"({i}) {url}\"\n",
        "\n",
        "        if not df.empty and url in df[\"Url\"].values:\n",
        "            logging.info(f\"Skipping already parsed {display_name}\")\n",
        "        elif dictionary := scrape_url(url):\n",
        "            df = append_to_dataset(dictionary, df)\n",
        "            logging.info(f\"Done parsing {display_name}\")\n",
        "            if len(df) % CHECKPOINT_RATE == 0:\n",
        "                save_checkpoint(df)\n",
        "        else:\n",
        "            logging.warning(f\"Giving up, failed to parse {display_name}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(logging)\n",
        "logging.basicConfig(\n",
        "    format=\"[%(asctime)s] %(levelname)-8s %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.DEBUG,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-01-21 12:44:22] INFO     Read 0 URLs to scrape\n",
            "[2023-01-21 12:44:22] INFO     Reloaded 12229 entries from disk\n",
            "[2023-01-21 12:44:22] INFO     Have 12229 entries in total\n",
            "[2023-01-21 12:44:22] INFO     Done saving everything\n"
          ]
        }
      ],
      "source": [
        "# Load all the URLs we want to parse.\n",
        "urls = list(pd.read_csv(LINKS_FILE)[\"link-href\"])[LINKS_START:LINKS_STOP + 1]\n",
        "logging.info(f\"Read {len(urls)} URLs to scrape\")\n",
        "\n",
        "# Reload the previous scraped results if they exist.\n",
        "df = load_checkpoint() if checkpoint_exists() else pd.DataFrame()\n",
        "logging.info(f\"Reloaded {len(df)} entries from disk\")\n",
        "\n",
        "# Add all the new entries to the dataset.\n",
        "df = scrape_all(df, urls)\n",
        "logging.info(f\"Have {len(df)} entries in total\")\n",
        "\n",
        "# Save the final results.\n",
        "save_checkpoint(df)\n",
        "logging.info(\"Done saving everything\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
