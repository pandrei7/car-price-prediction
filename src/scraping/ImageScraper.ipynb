{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autovit Image Scraper\n",
        "\n",
        "This notebook can scrape car images from a fixed list of URLs.\n",
        "\n",
        "Each URL should point to the \"offer web page\" of a car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import pandas as pd\n",
        "import ratelimit\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The file which contains the URLs to scrape.\n",
        "LINKS_FILE = os.path.join(\"..\", \"..\", \"data\", \"carsWithImages.csv\")\n",
        "# The directory where output is saved.\n",
        "OUTPUT_DIR = os.path.join(\"..\", \"..\", \"data\", \"autovit_images\")\n",
        "\n",
        "# The first and last URLs to scrape (indices inside the URL list).\n",
        "LINKS_START = 0\n",
        "LINKS_STOP = 2\n",
        "\n",
        "# Maximum number of requests in a `LIMIT_PERIOD` period.\n",
        "LIMIT_CALLS: int = 1\n",
        "# Duration of a period, in seconds.\n",
        "LIMIT_PERIOD: int = 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for downloading data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_webpage(url: str) -> str:\n",
        "    \"\"\"Download a text page from a given URL.\"\"\"\n",
        "    resp = urlopen(url).read().decode('utf-8')\n",
        "    return resp\n",
        "\n",
        "\n",
        "def download_image(url: str) -> bytes:\n",
        "    \"\"\"Download the raw bytes of an image from a given URL.\"\"\"\n",
        "    return urlopen(url).read()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Requests are rate-limited to avoid overwhelming the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "@ratelimit.sleep_and_retry\n",
        "@ratelimit.limits(calls=LIMIT_CALLS, period=LIMIT_PERIOD)\n",
        "def check_autovit_limit() -> None:\n",
        "    \"\"\"\n",
        "    Utility function which forces requests to Autovit to respect rate limits.\n",
        "    This function should be used as a \"guard\", simply call it before you make a\n",
        "    request.\n",
        "    \"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for parsing the pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CarInfo:\n",
        "    \"\"\"Contains metadata about a car.\"\"\"\n",
        "    id: str\n",
        "    img_url: Optional[str]\n",
        "    name: str\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Image:\n",
        "    \"\"\"Wraps an image and its metadata.\"\"\"\n",
        "    image: bytes\n",
        "    info: CarInfo\n",
        "\n",
        "\n",
        "def parse_webpage(html: str) -> CarInfo:\n",
        "    \"\"\"Parse car metadata from an HTML page.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    autovit_id = soup.find(\"span\", {\"id\": \"ad_id\"}).text\n",
        "\n",
        "    name = soup.find(\"span\", {\"class\": \"offer-title\"}).text.strip()\n",
        "\n",
        "    # Image URLs seem to contain the resolution of the image as a suffix of the\n",
        "    # URL. If we remove this, suffix we should have access to the full image.\n",
        "    img_size_pattern = r\";s=\\d+x\\d+$\"\n",
        "    img_tag = soup.find(\"div\", {\"class\": \"photo-item\"}).find(\"img\")\n",
        "    img_url = re.sub(img_size_pattern, \"\", img_tag[\"data-lazy\"])\n",
        "\n",
        "    return CarInfo(id=autovit_id, img_url=img_url, name=name)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for storing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_dir(autovit_id: str) -> str:\n",
        "    \"\"\"Returns the path of the directory where an article's info is saved.\"\"\"\n",
        "    return os.path.join(OUTPUT_DIR, autovit_id)\n",
        "\n",
        "\n",
        "def done_path(autovit_id: str) -> str:\n",
        "    \"\"\"Return the path of the file that says an article has been processed.\"\"\"\n",
        "    return os.path.join(save_dir(autovit_id), \".done\")\n",
        "\n",
        "\n",
        "def already_scraped(autovit_id: str) -> bool:\n",
        "    \"\"\"Check if a given article can already be found on disk.\"\"\"\n",
        "    return os.path.isfile(done_path(autovit_id))\n",
        "\n",
        "\n",
        "def save_to_disk(image: Image) -> None:\n",
        "    \"\"\"Save all information related to an image to disk.\"\"\"\n",
        "    save_path = save_dir(image.info.id)\n",
        "\n",
        "    # Make sure the directory exists.\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Save the image to disk.\n",
        "    img_path = os.path.join(save_path, f\"{image.info.id}.webp\")\n",
        "    with open(img_path, \"wb\") as fout:\n",
        "        fout.write(image.image)\n",
        "\n",
        "    # Remember that we processed this article.\n",
        "    with open(done_path(image.info.id), \"w\") as fout:\n",
        "        fout.write(image.info.name)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The actual scraping process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def obtain_image(webpage_url: str) -> Optional[Image]:\n",
        "    \"\"\"Try to download the image and metadata which corresponds to a car.\"\"\"\n",
        "    try:\n",
        "        check_autovit_limit()\n",
        "        html = download_webpage(webpage_url)\n",
        "        info = parse_webpage(html)\n",
        "\n",
        "        check_autovit_limit()\n",
        "        img = download_image(info.img_url)\n",
        "\n",
        "        return Image(image=img, info=info)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "def scrape_all(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Scrape all images from the given collection, logging the progress.\"\"\"\n",
        "    for i, (_, entry) in enumerate(df.iterrows()):\n",
        "        autovit_id = str(entry[\"Autovit Id\"])\n",
        "        webpage_url = entry[\"Url\"]\n",
        "        display_name = f\"({i}) {autovit_id} {webpage_url}\"\n",
        "\n",
        "        if already_scraped(autovit_id):\n",
        "            logging.info(f\"Skipping already scraped {display_name}\")\n",
        "            continue\n",
        "\n",
        "        if image := obtain_image(webpage_url):\n",
        "            save_to_disk(image)\n",
        "            logging.info(f\"Done processing {display_name}\")\n",
        "        else:\n",
        "            logging.warning(f\"Giving up, failed to parse {display_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(logging)\n",
        "logging.basicConfig(\n",
        "    format=\"[%(asctime)s] %(levelname)-8s %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.DEBUG,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-01-21 15:42:00] INFO     Read 3 entries to scrape\n",
            "[2023-01-21 15:42:00] INFO     Skipping already scraped (0) 7049990250 https://www.autovit.ro/anunt/suzuki-vitara-1-6-ID7H72va.html\n",
            "[2023-01-21 15:42:00] INFO     Skipping already scraped (1) 7049960669 https://www.autovit.ro/anunt/toyota-auris-1-8-vvt-i-hybrid-automatik-touring-sports-ID7H6UP3.html\n",
            "[2023-01-21 15:42:00] INFO     Skipping already scraped (2) 7049895868 https://www.autovit.ro/anunt/skoda-octavia-1-6-tdi-ID7H6DXS.html\n",
            "[2023-01-21 15:42:00] INFO     Done processing everything\n"
          ]
        }
      ],
      "source": [
        "# Load only the entries we want to parse.\n",
        "df = pd.read_csv(LINKS_FILE).iloc[LINKS_START:LINKS_STOP + 1]\n",
        "logging.info(f\"Read {len(df)} entries to scrape\")\n",
        "\n",
        "# Actually scrape the images.\n",
        "scrape_all(df)\n",
        "logging.info(\"Done processing everything\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
